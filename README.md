# ğŸ” K-Nearest Neighbors (KNN) Algorithm

**K-Nearest Neighbors (KNN)** is a simple, versatile, and widely used **supervised machine learning algorithm** used for both **classification** and **regression** tasks. However, it is **more commonly used for classification**.

KNN is a **non-parametric**, **instance-based (lazy learning)** algorithm:
- It **makes no assumptions** about the underlying data distribution.
- It **does not learn** a discriminative function from the training data.
- Instead, it **memorizes the dataset** and makes predictions based on the **'k' closest training examples** in the feature space.

---

## âš™ï¸ How It Works

1. **Choose** the number of neighbors \( k \).
2. **Calculate** the distance between the new input and all training samples (commonly **Euclidean distance**).
3. **Sort** the distances and find the **k nearest neighbors**.
4. **Make predictions**:
   - **Classification**: Use **majority voting** among the k neighbors.
   - **Regression**: Take the **average** (or **weighted average**) of the k neighborsâ€™ values.

---

## ğŸ”‘ Key Parameters

- **`n_neighbors`**: Number of neighbors to consider (**k**).  
  â†’ Higher k reduces noise but may blur class boundaries.

- **`weights`**:  
  â†’ `'uniform'`: All neighbors have equal weight  
  â†’ `'distance'`: Closer neighbors have more influence

- **`algorithm`**:  
  â†’ `'auto'`, `'ball_tree'`, `'kd_tree'`, or `'brute'`  
  â†’ Specifies the method used to compute nearest neighbors

- **`p`**:  
  â†’ Power parameter for **Minkowski distance**  
  â†’ `p=1` â†’ Manhattan distance  
  â†’ `p=2` â†’ Euclidean distance

- **`metric`**:  
  â†’ Distance metric to use (default: `'minkowski'`)  
  â†’ Can also define **custom metrics**

---

## ğŸ“ˆ Evaluating KNN Performance

### Classification Metrics
- **Accuracy**
- **Precision**
- **Recall**
- **F1-score**
- **Confusion Matrix**
- **ROC-AUC Curve**

### Regression Metrics
- **Mean Squared Error (MSE)**
- **Mean Absolute Error (MAE)**
- **RÂ² Score** (Coefficient of Determination)

---

## ğŸ” Cross-Validation

Use **K-Fold Cross-Validation** to:
- Select the **optimal value of k**
- Avoid **overfitting** and **underfitting**

---

## ğŸ§  Choosing the Best `k`

- **Small `k`** â†’ More sensitive to noise â†’ **High variance**
- **Large `k`** â†’ Smoother decision boundary â†’ **High bias**
- Use **Grid Search** with **Cross-Validation** to find the **optimal k**

---

## âš ï¸ Limitations

- âš¡ **Slow at prediction time** for large datasets (since it scans all data)
- â— **Sensitive** to irrelevant or redundant features
- ğŸ“‰ Performance **degrades with high-dimensional data** (curse of dimensionality)
- ğŸ”§ **Requires feature scaling** (e.g., `StandardScaler` or `MinMaxScaler`)

